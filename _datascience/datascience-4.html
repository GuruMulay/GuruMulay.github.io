---
title: "Hadoop MapReduce: Content Based Authorship Detection using TF/IDF Scores and
Cosine Similarity"
excerpt: "<br/><img src='/images/csu/proj/big-data-1/ngram1.png'> <br/> <small> </small> "
collection: datascience
tags:
  - Hadoop
  - map-reduce
  - word-count
  - authorship-detection
  - BigData
  - tf-idf-scores
---

<hr style="border-color: silver;">
<a href="#" ><button type="button" class="btn btn--info"><i class="fa fa-github"></i> GitHub Source</button></a>
<!--<i class="fa fa-file-code-o"></i>-->
<!--<a href="#" class="btn btn&#45;&#45;inverse">Inverse Button</a>-->
<!--<a href="#" class="btn btn&#45;&#45;info">Info Button</a>-->
<!--<iframe src="https://ghbtns.com/github-btn.html?user=gurumulay&repo=big-data-class/tree/master/n-gram-analysis-of-gutenberg&type=star&count=false&size=large" frameborder="0" scrolling="0" width="160px" height="30px"></iframe>-->
<hr style="border-color: silver;">


<!--##############-->
<!--<hr>-->
<p>
    <big>
    <b>Introduction:</b>
    </big>
</p>


<p>
    The goal is to build an authorship detection system that provides a ranked list of
    possible authors for a document whose authorship is unknown. The system uses N-gram analysis from
    <a href="/datascience/datascience-3/">this project</a>. In particular, I used the word uni-grams from that project.

    Finding the attributes of literature content written by an author is important to detect unique
    writing style for that author. I built an attribute vector for each of the authors with the help of
    <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">Term Frequency and Inverse Document Frequency (TF-IDF)</a>
    analysis. Authorship is determined based on the similarity between the attribute vectors of the unseen document and
    the pre-computed attribute vectors of authors. TF-IDF is used to calculate the weights of each of the entities in
    the attribute vector.


</p>


<!--<p>-->
<!--<code style="font-size: 105%;">Let's study Big Data</code>-->
<!--</p>-->

<!--<br style="margin-bottom:25px;"/>-->

<!--<p>-->
    <!--The 1-grams for this sentence are <code>["Let's", "study", "Big", "Data"]</code>. The 2-grams are-->
    <!--<code>["__, Let's", "Let's, study", "study, Big", "Big, Data", "Data, __"]</code>.-->
    <!--Here, "__" represents the empty space before and after the sentence which helps to distinguish whether the word was-->
    <!--used in the beginning or the end of the sentence or whether word was paired with other words.-->
<!--</p>-->


<!--##############-->
<hr>
<p>
    <big>
    <b>Definitions:</b>
    </big>
</p>


<p>
    <b>Term Frequency:</b>
</p>

<p>
    Let's assume that we have a collection of documents written by M authors. This collection of documents may
    contain multiple books written by an author. Let’s define a set of documents written by same author as
    a sub-collection j. We define f ij to be the frequency (Number of occurrences) of term (word) i in sub-
    collection j.

    <br/>
    <code>
    TF ij =0.5+0.5(f ij /max k f kj )
    </code>
    <br/>
    
    We use the augmented TF to prevent a bias towards longer documents. E.g. raw
    frequency divided by the maximum raw frequency of any term k in the sub-collection j. Stop words were not eliminated
    from the corpus. The most frequent term in the sub-collection will have a augmented TF value of 1.

</p>


<p>
    <b>Inverse Document Frequency:</b>
</p>


<p>
    <b>TF.IDF value:</b>
</p>


<!--##############-->
<hr>
<p>
    <big>
    <b>Introduction:</b>
    </big>
</p>

<p>
    For this project, the goal was to create an N-gram profile of a corpus of modern English literature formed by
    subsetting around 1GB of dataset that included more than 2500 books from Gutenberg project [2]. Major steps
    involved: (1) Extracting all the unigrams and bigrams, (2) Computing the frequency of each unigram and bigram, and (3) Ranking
    the unigrams and bigrams based on those frequencies. I used
    <a href="https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html">Hadoop's</a>
    <a href="https://en.wikipedia.org/wiki/MapReduce">MapReduce</a>
    <a href=""></a> framework to generate the ngram profiles.

    <!--<div style="text-align: center">-->
        <!--<a href="/images/csu/proj/sotorrent/so_stats.png">-->
            <!--<img src="/images/csu/proj/sotorrent/so_stats.png" alt="SOTorrent statistics" align="middle" width="500">-->
        <!--</a>-->
        <!--<br/>-->
        <!--<br/>-->
        <!--<figcaption>Fig. 1: SOTorrent stats</figcaption>-->
    <!--</div>-->
    <!--<br/>-->

</p>

<!--##############-->
<hr>
<p>
    <big>
    <b>Dataset:</b>
    </big>
</p>

<p>
    Dataset - which was provided to us during the class - was compiled from the set of EBook files available as a part
    of Project Gutenberg. It was a small subset of size ~1gb consisting of lines from several books from the
    Project Gutenberg. Each line looked as follows:

    <br style="margin-bottom:15px;"/>
    <code>AUTHOR<===>DATE<===>A SINGLE LINE FROM THE DATASET FILE</code>
    <br style="margin-bottom:15px;"/>


    A specific example would be:

    <br style="margin-bottom:15px;"/>
    <code>Arthur Conan Doyle<===>June 19, 2008<===>"Well, by his insufferable rudeness and impossible behavior."</code>
    <br style="margin-bottom:15px;"/>

    For this project we were instructed to work with author's last name from <code>AUTHOR</code> field (last author in
    case of multiple authors) and only the year part of <code>DATE</code> field. We converted the words into lowercase
    to make them case insensitive. All the gender and tense identifier are treated as a separate word. Meaning "he" and
    "she" were treated separately. Words "have" and "has" were treated as two separate words.
</p>



<!--##############-->
<hr>
<p>
    <big>
    <b>Methodology:</b>
    </big>
</p>


<p>
THe ngram profile analysis generates following:

<ol type="1">

    <li><strong>Profile 1-A:</strong> A list of unigrams sorted by unigrams in an alphabetical ascending order. Within the same unigram, the list
    should be sorted by year in an ascending order.</li>

    <li><strong>Profile 1-B:</strong> A list of unigrams sorted by unigrams in an alphabetical ascending order. Within the same unigram, the list
    should be sorted by author’s last name in an ascending order.</li>

    <li><strong>Profile 2-A:</strong> A list of bigrams sorted by bigrams in an alphabetical ascending order. Within the same bigram, the list
    should be sorted by year in an ascending order.</li>

    <li><strong>Profile 2-B:</strong> A list of bigrams sorted by bigrams in an alphabetical ascending order. Within the same bigram, the list
    should be sorted by author's last name in an ascending order.</li>

</ol>

</p>


<!--##############-->
<hr>
<p>
    <big>
    <b>Outputs:</b>
    </big>
</p>

Here are some snippets from the outputs of map reduce program for four profiles mentioned above. If you are interested
you could get the entire files from a cloud bucket as below:



<br/>
<a href="
https://project-data-store-public.storage.googleapis.com/csu-proj-data/big-data-stuff/guten1A"> Profile-1A,
</a>

<a href="
https://project-data-store-public.storage.googleapis.com/csu-proj-data/big-data-stuff/guten1B"> Profile-1B,
</a>

<a href="
https://project-data-store-public.storage.googleapis.com/csu-proj-data/big-data-stuff/guten2A"> Profile-2A,
</a> and

<a href="
https://project-data-store-public.storage.googleapis.com/csu-proj-data/big-data-stuff/guten2B"> Profile-2B.
</a>

And the <a href="">source</a> as well.



<!--<a href="https://raw.githubusercontent.com/GuruMulay/big-data-class/master/n-gram-analysis-of-gutenberg/guten1A.png"><img  alt="gutenberg ngram analysis using hadoop map-reduce" src="https://raw.githubusercontent.com/GuruMulay/big-data-class/master/n-gram-analysis-of-gutenberg/guten1A.png" style="float: left; width: 40%; margin-right: 1%; margin-bottom: 0.5em;"  ></a>-->
<!--<a href="https://raw.githubusercontent.com/GuruMulay/big-data-class/master/n-gram-analysis-of-gutenberg/guten1B.png"><img  alt="gutenberg ngram analysis using hadoop map-reduce" src="https://raw.githubusercontent.com/GuruMulay/big-data-class/master/n-gram-analysis-of-gutenberg/guten1B.png" style="float: left; width: 40%; margin-right: 1%; margin-bottom: 0.5em;"  ></a>-->


<!--<a href="https://raw.githubusercontent.com/GuruMulay/big-data-class/master/n-gram-analysis-of-gutenberg/guten2A.png"><img  alt="gutenberg ngram analysis using hadoop map-reduce" src="https://raw.githubusercontent.com/GuruMulay/big-data-class/master/n-gram-analysis-of-gutenberg/guten2A.png" style="float: left; width: 48%; margin-right: 1%; margin-bottom: 0.5em;"  ></a>-->
<!--<a href="https://raw.githubusercontent.com/GuruMulay/big-data-class/master/n-gram-analysis-of-gutenberg/guten2B.png"><img  alt="gutenberg ngram analysis using hadoop map-reduce" src="https://raw.githubusercontent.com/GuruMulay/big-data-class/master/n-gram-analysis-of-gutenberg/guten2B.png" style="float: left; width: 48%; margin-right: 1%; margin-bottom: 0.5em;"  ></a>-->

<!--<figcaption>Fig. 2: BigQuery Interface</figcaption>-->




<div style="text-align: center">
    <a href="https://raw.githubusercontent.com/GuruMulay/big-data-class/master/n-gram-analysis-of-gutenberg/guten1A.png">
        <img src="https://raw.githubusercontent.com/GuruMulay/big-data-class/master/n-gram-analysis-of-gutenberg/guten1A.png" alt="gutenberg ngram analysis using hadoop map-reduce" align="middle" hspace="50" height="350">
    </a>

    <a href="https://raw.githubusercontent.com/GuruMulay/big-data-class/master/n-gram-analysis-of-gutenberg/guten1B.png">
        <img src="https://raw.githubusercontent.com/GuruMulay/big-data-class/master/n-gram-analysis-of-gutenberg/guten1B.png" alt="gutenberg ngram analysis using hadoop map-reduce" align="middle" hspace="50" height="350">
    </a>
    <br/>
    <br/>
    <figcaption>Fig. 1: Profile 1A and 1B sample outputs</figcaption>
</div>
<br/>


<br style="margin-bottom:10px;"/>
<div style="text-align: center">
    <a href="https://raw.githubusercontent.com/GuruMulay/big-data-class/master/n-gram-analysis-of-gutenberg/guten2A.png">
        <img src="https://raw.githubusercontent.com/GuruMulay/big-data-class/master/n-gram-analysis-of-gutenberg/guten2A.png" alt="gutenberg ngram analysis using hadoop map-reduce" align="middle" hspace="30" height="350">
    </a>

    <a href="https://raw.githubusercontent.com/GuruMulay/big-data-class/master/n-gram-analysis-of-gutenberg/guten2B.png">
        <img src="https://raw.githubusercontent.com/GuruMulay/big-data-class/master/n-gram-analysis-of-gutenberg/guten2B.png" alt="gutenberg ngram analysis using hadoop map-reduce" align="middle" hspace="30" height="350">
    </a>
    <br/>
    <br/>
    <figcaption>Fig. 1: Profile 2A and 2B sample outputs</figcaption>
</div>
<br/>





<!--##############-->
<hr>
<p>
    <big>
    <b>References:</b>
    </big>
</p>


<ol type="1">
    <li>https://en.wikipedia.org/wiki/N-gram</li>
    <li>https://www.gutenberg.org</li>
</ol>



<!--##############-->
<hr>
<p>
    <small>
    <b>Note:</b>
    Some of the content for this post is taken from course material of CS435 (CSU) written by Prof. Sangmi Lee Pallickara and GTAs.
    </small>
</p>
<hr>