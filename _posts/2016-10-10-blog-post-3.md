---
title: 'What I learned while hand-labeling an extensive video dataset of gestures'
date: 2016-10-10
permalink: /posts/2016/10/blog-post-3/
tags:
  - labeling
  - gesture-dataset
  - machine-learning
  - computer-vision
---

In 2017 [CSU computer vision lab](https://www.cs.colostate.edu/~vision/) released a dataset (EGGNOG [[1]](https://ieeexplore.ieee.org/abstract/document/7961771/) or [[2]](https://www.cs.colostate.edu/~vision/eggnog/papers/EGGNOG_FG2017.pdf)) of gestures containing ~8 hours of videos of labeled gestures.
The dataset is called EGGNOG which stands for _Elicited Giant Gallery of Naturally Occurring Gestures_.

Before we dive further, if you are interested in the details of the dataset please read [this]() post.


Before I begin sharing my first-hand experience in manually labeling a

Headings are cool
======

You can have many headings
======

Aren't headings cool?
------