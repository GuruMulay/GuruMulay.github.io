---
title: "EGGNOG: A continuous, multi-modal data set of naturally
occurring gestures with ground truth labels"
excerpt: "<img src='/images/vision/eggnog/sample2.jpg'>"
collection: vision
tags:
  - gesture-dataset
  - naturally-occurring-gestures
  - depth-data
  - skeleton-data
  - EGGNOG-dataset
---

-------
<p><span style="color: #501070">By: Isaac Wang, Mohtadi Ben Fraj, Pradyumna Narayana, Dhruva Patil, Gururaj Mulay, Rahul Bangar, J. Ross Beveridge, Bruce A. Draper, Jaime Ruiz<br/></span>
<span style="color: #905050">Colorado State University, University of Florida<br/></span>
<span style="color: #AA7030"> 2017 IEEE 12th International Conference on Automatic Face & Gesture Recognition (FG)</span>.</p>


[<i class="fa  fa-file-text-o"></i> Paper](https://gurumulay.github.io/files/papers/EGGNOG_FG2017.pdf){: .btn .btn--success} &nbsp; &nbsp; [<i class="fa fa-fw fa-download"></i> Dataset](https://www.cs.colostate.edu/~vision/eggnog/){: .btn .btn--success}


-------


People communicate through words and gestures,
but current voice-based computer interfaces such as Siri exploit
only words. This is a shame: human-computer interfaces would
be natural if they incorporated gestures as well as words. To
support this goal, we present a new dataset of naturally occurring
gestures made by people working collaboratively on blocks world
tasks. The dataset, called EGGNOG, contains over 8 hours of
RGB video, depth video, and Kinect v2 body position data of 40
subjects. The data has been semi-automatically segmented into
24,503 movements, each of which has been labeled according
to (1) its physical motion and (2) the intent of the participant.
We believe this dataset will stimulate research into natural and
gestural human-computer interfaces.


Here are a few sample frames from the dataset:

<figure>
	<a href="/images/vision/eggnog/sample1.jpg"><img src="/images/vision/eggnog/sample1.jpg"></a>
	<figcaption><a href="" title="">A sample frame from EGGNOG dataset</a>.</figcaption>
</figure>


The goal of this data set is to support research into recognizing
the types of gestures that occur during human communication.
The data set, including video, depth, and body pose data,
is publicly available [here](https://cwc.cs.colostate.edu/datasets),
along with the corresponding segment boundaries and ground
truth labels.