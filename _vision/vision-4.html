---
title: "Attention Window and Object Detection with Tracking"
excerpt: "<br/><img src='/images/vision/image-computation/img-comp.png'>"
collection: vision
tags:
  - object-detection
  - object-tracking
  - attention-window-detection
  - SIFT-features
  - MOSSE-filter
  - saliency-maps
  - OpenCV
---


<hr style="border-color: silver;">
<a href="https://github.com/bangar1991/cs510"><button type="button" class="btn btn--info"><i class="fa fa-github"></i> GitHub Source</button></a>
Team Size: 3
<hr style="border-color: silver;">




<!--&lt;!&ndash;##############&ndash;&gt;-->
<!--<p>-->
    <!--<big>-->
    <!--<b>Introduction:</b>-->
    <!--</big>-->
<!--</p>-->

<!--<strong> Brief intro to CwC:</strong>-->
<!--<br style="margin-bottom:10px;"/>-->

<!--<p>-->
    <!--The long term goal of the <a href="https://www.cs.colostate.edu/~draper/home_research.php">CwC project</a>-->
    <!--is to build an AI agent that will communicate seamlessly with humans via gestures, expressions, and vocal communication-->
    <!--just like a real human being would with another human being. The agent achieves this communication by building a shared-->
    <!--perception with the humans using all the knowledge it acquired through all the past and current interactions with the humans.-->
    <!--The short term goal is-->
    <!--to get a structure of virtual blocks build from a virtual AI agent (Diana) via multi-modal communication.-->
    <!--CSU's computer vision lab (of which I am a part of) has built this system (please see the demo video below) as-->
    <!--a part of an active (as of 2018) DARPA project called CwC.-->
<!--</p>-->


<!--<p>-->
    <!--[[Video]]-->
<!--</p>-->

<!--<strong> RGB-based Skeleton Recognition for CwC:</strong>-->
<!--<br style="margin-bottom:10px;"/>-->

<!--<p>-->
    <!--Recent work by CMU [1], [2] in human pose recognition from RGB images, produces state-of-the-art-->
    <!--results on multiple datasets. Success of these methods-->
    <!--can be primarily attributed to convolutional neural-->
    <!--networks for joint detection and special techniques-->
    <!--to associate detected joints with people in a frame.-->
    <!--We incorporated this work to replace previous-->
    <!--depth-based pose recognition. Task involved writing a C++ wrapper around the OpenPose library-->
    <!--[1] and a Python server to interface this wrapper-->
    <!--with the rest of the clientâ€™s system, along with some-->
    <!--modifications to previous clients-->
<!--</p>-->


<!--<p>-->
    <!--As a part of my <a href="https://www.cs.colostate.edu/~cs793/Fall17/">Research Seminar Course (CSU CS793)</a>, I-->
    <!--worked on the front-end of the CwC project to replace the Kinect-based data collection module at the front-end-->
    <!--of the Communicating with Computers project with an RGB camera-based module in a seamless manner with following-->
    <!--objectives:-->

<!--<ul>-->
    <!--<li>Retrieve the pose skeleton keypoints of a human subject from RGB images.</li>-->
    <!--<li>Transmit keypoints & RGB images of hands, head to clients for gesture recognition.</li>-->
    <!--<li>Write a wrapper for <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose library </a>-->
        <!--that does pose skeleton detection.</li>-->
<!--</ul>-->

<!--</p>-->


<!--<p>-->
    <!--&lt;!&ndash;Approximate pipeline of the CwC project:&ndash;&gt;-->
    <!--<br style="margin-bottom:10px;"/>-->
    <!--<div style="text-align: center">-->
        <!--<a href="https://raw.githubusercontent.com/GuruMulay/rgb-based-skeleton-recognition-for-cwc-proj/master/media/cwc3.png">-->
            <!--<img src="https://raw.githubusercontent.com/GuruMulay/rgb-based-skeleton-recognition-for-cwc-proj/master/media/cwc3.png" alt="CwC project block diagram" align="middle" hspace="30" height="300">-->
        <!--</a>-->
        <!--<br/>-->
        <!--<figcaption>Fig. 1: CwC project pipeline</figcaption>-->
    <!--</div>-->
    <!--<br/>-->

<!--</p>-->


<!--<p>-->
    <!--Here is the comparison between old and new front-end for our CwC pipeline.-->
    <!--<br style="margin-bottom:10px;"/>-->
    <!--<div style="text-align: center">-->
        <!--<a href="https://raw.githubusercontent.com/GuruMulay/rgb-based-skeleton-recognition-for-cwc-proj/master/media/diff.png">-->
            <!--<img src="https://raw.githubusercontent.com/GuruMulay/rgb-based-skeleton-recognition-for-cwc-proj/master/media/diff.png" alt="CwC project old vs new front-end" align="middle" hspace="30" height="300">-->
        <!--</a>-->
        <!--<br/>-->
        <!--<figcaption>Fig. 2: CwC: old vs new front-end</figcaption>-->
    <!--</div>-->
    <!--<br/>-->

<!--</p>-->



<!--##############-->
<!--<hr>-->
<p>
    <big>
    <b>Objectives:</b>
    </big>
</p>

This project contains a series of assignments put together to build a final project with a goal of object detection and
tracking. Here are the series of sub-tasks constituting towards the final goal:

<ol>
    <li><b>Affine and Perspective transforms on a video:</b>
        <a href="https://github.com/GuruMulay/cs510-mirror/tree/master/a1">This code</a> performs either affine or perspective transformation on an input video using OpenCV's built-in functions.
    </li>


    <li><b>Extract unique attention windows from each frame of a video:</b>
        <a href="https://github.com/GuruMulay/cs510-mirror/tree/master/a2">This code</a> extracts attention windows from an input video.
    </li>


    <li><b>Detect and Track moving objects:</b>
        <a href="https://github.com/GuruMulay/cs510-mirror/tree/master/a3">This code</a> implements object detection and tracking on a video.
    </li>


    <li>For every frame processed by OpenPose, the
server sends corresponding data to four clients.</li>

</ol>




<!--##############-->
<hr>
<p>
    <big>
    <b>Extract unique attention windows:</b>
    </big>
</p>


<p>
    Objective is to write a program that reads a video and produces one attention window per frame.
    The window represents the 'best' attention window for that frame and is written back to the disk.
    We implemented multiple ways to find the best attention window per frame. Some of the approaches were:

    <ul>

    <li>SIFT keypoints based approach that finds the best attention window using size-response sorting</li>

    <li>SIFT keypoints clustering using hierarchical (HCA) and DBSCAN clustering algorithms</li>

    <li>Saliency based attention models (Boolean Map based Saliency)</li>

    </ul>

    Figure 1 and 2 show example frames from a video. Figure 1 shows keypoints clustered using hierarchical clustering
    algorithm. Clusters form an attention window. Figure 2 shows saliency map for a frame from the video. High saliency
    means the region should qualify for high attention.


    <br/>
    <br style="margin-bottom:20px;"/>
    <a href="https://github.com/GuruMulay/cs510-mirror/blob/master/a2/assignment2-report.pdf"><button type="button" class="btn btn--info"><i class="fa fa-file-pdf-o"></i> PDF Report</button></a>
    - Here is a short report detailing about each of the approaches mentioned above.


    <br/>
    <br style="margin-bottom:20px;"/>
    <div style="text-align: center">
        <a href="/images/vision/image-computation/clustering.png">
            <img src="/images/vision/image-computation/clustering.png" alt="Attention widows based on hierarchical clustering" align="middle" hspace="0" height="300">
        </a>
        <br/>
        <br/>
        <figcaption>Fig. 1: Attention widows based on hierarchical clustering</figcaption>
    </div>

    <br style="margin-bottom:10px;"/>
    <div style="text-align: center">
        <a href="/images/vision/image-computation/saliency.png">
            <img src="/images/vision/image-computation/saliency.png" alt="Attention widows based on saliency maps" align="middle" hspace="0" height="300">
        </a>
        <br/>
        <br/>
        <figcaption>Fig. 2: Attention widows based on saliency maps</figcaption>
    </div>

</p>



<!--##############-->
<hr>
<p>
    <big>
    <b>Detect and Track moving objects:</b>
    </big>
</p>


    <br/>
    <a href="https://github.com/GuruMulay/cs510-mirror/blob/master/a3/assignment3-report.pdf"><button type="button" class="btn btn--info"><i class="fa fa-file-pdf-o"></i> PDF Report</button></a>
    - Here is a short report on object detection and tracking.
    <br/>


<br style="margin-bottom:10px;"/>

<iframe width="560" height="315" src="https://www.youtube.com/embed/R-waR3JWDIY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
    <p>This browser does not support embedded YouTube videos. Please visit <a href="https://www.youtube.com/embed/R-waR3JWDIY"> here for this YouTube video.</a>.</p>
</iframe>

<br style="margin-bottom:10px;"/>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Ajp-H0SiAGo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
    <p>This browser does not support embedded YouTube videos. Please visit <a href="https://www.youtube.com/embed/Ajp-H0SiAGo"> here for this YouTube video.</a>.</p>
</iframe>



<!--##############-->
<hr>
<p>
    <big>
    <b>PA5:</b>
    </big>
</p>


    <br/>
    <a href="https://github.com/GuruMulay/cs510-mirror/blob/master/a5/assignment5-report.pdf"><button type="button" class="btn btn--info"><i class="fa fa-file-pdf-o"></i> PDF Report</button></a>
    - Here is a short report on this assignment.
    <br/>


<br style="margin-bottom:10px;"/>

<iframe width="560" height="315" src="https://www.youtube.com/embed/IqFAqBp9Tzc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
    <p>This browser does not support embedded YouTube videos. Please visit <a href="https://www.youtube.com/embed/IqFAqBp9Tzc"> here for this YouTube video.</a>.</p>
</iframe>




<!--&lt;!&ndash;##############&ndash;&gt;-->
<!--<hr>-->
<!--<p>-->
    <!--<big>-->
    <!--<b>References:</b>-->
    <!--</big>-->
<!--</p>-->

<!--<ol type="1">-->
    <!--<li>Openpose Library: github.com/CMU-Perceptual-Computing-Lab/openpose</li>-->
    <!--<li>Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields, Z Cao, T Simon, S Wei, Y Sheikh,-->
        <!--arXiv:1611.08050, 2016</li>-->
<!--</ol>-->


